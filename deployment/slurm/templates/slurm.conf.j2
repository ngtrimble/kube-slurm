# slurm.conf
#
# See the slurm.conf man page for more information.
#
ClusterName=deepops
ControlMachine=slurmctld
ControlAddr=slurmctld

# RGK 2023/02/28 - only accept jobs which don't exceed resources
EnforcePartLimits=ALL

# RGK 2023/04/03 - allow for interactive jobs (plugin lua determines these by absence of batch script) on some
# Lambda nodes
JobSubmitPlugins=lua
LaunchParameters=use_interactive_step

#BackupController=
#BackupAddr=
#
SlurmUser=slurm
#SlurmdUser=root
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
StateSaveLocation=/var/lib/slurmd
SlurmdSpoolDir=/var/spool/slurmd
SwitchType=switch/none
MpiDefault=none
SlurmctldPidFile=/var/run/slurmctld/slurmctld.pid
SlurmdPidFile=/var/run/slurmd/slurmd.pid
ProctrackType=proctrack/linuxproc
#PluginDir=
#CacheGroups=0
FirstJobId=30001
ReturnToService=0
#MaxJobCount=
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
# 2022/08/11 RGK
PropagateResourceLimits=NONE
#PropagateResourceLimitsExcept=
#Prolog=
#Epilog=
#SrunProlog=
#SrunEpilog=
#TaskProlog=
#TaskEpilog=
#TaskPlugin=
#TrackWCKey=no
#TreeWidth=50
#TmpFS=
#UsePAM=
#
# TIMERS
SlurmctldTimeout=300
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0
#



SchedulerType=sched/backfill
#SchedulerAuth=
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory,CR_CORE_DEFAULT_DIST_BLOCK,CR_ONE_TASK_PER_CORE
FastSchedule=1
# RGK 2022/02/17
# JOB PRIORITY
#PriorityFlags=NO_FAIR_TREE
PriorityType=priority/multifactor
PriorityWeightFairshare=1000
PriorityCalcPeriod=5
#PriorityFavorSmall=
#PriorityDecayHalfLife=14-0
#PriorityUsageResetPeriod=14-0
#PriorityWeightFairshare=100000
#PriorityWeightAge=1000
#PriorityWeightPartition=10000
#PriorityWeightJobSize=1000
#PriorityMaxAge=1-0
#
#
# ACCOUNTING
JobAcctGatherType=jobacct_gather/linux
#JobAcctGatherFrequency=30
#
AccountingStorageTRES=gres/gpu
#DebugFlags=CPU_Bind,gres
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=slurmdbd
#AccountingStorageLoc=
#RGK 2022/11/30 - block job submission from unregistered users 
#AccountingStorageEnforce=associations,limits,qos
#Kalen 2023/2/14 - Change this to associations for kube-slurm
AccountingStorageEnforce=associations

#SchedulerAuth=
#SchedulerPort=
#SchedulerRootFilter=
#PriorityType=priority/multifactor
#PriorityDecayHalfLife=14-0
#PriorityUsageResetPeriod=14-0
#PriorityWeightFairshare=100000
#PriorityWeightAge=1000
#PriorityWeightPartition=10000
#PriorityWeightJobSize=1000
#PriorityMaxAge=1-0
#
# LOGGING
SlurmctldDebug=info
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=info
SlurmdLogFile=/var/log/slurm/slurmd.log
JobCompType=jobcomp/filetxt
JobCompLoc=/var/log/slurm/jobcomp.log
#
# ACCOUNTING
#
AccountingStoragePort=6819
#AccountingStorageLoc=slurm_acct_db
#AccountingStoragePass=
#AccountingStorageUser=
#
# COMPUTE NODES
#GresTypes=gpu
# COMPUTE NODES
GresTypes=gpu
NodeName=dgx-1  Gres=gpu:8     CPUs=80 Sockets=2 CoresPerSocket=20 ThreadsPerCore=2 Procs=40 RealMemory=489913 State=UNKNOWN
NodeName=dgx-2  Gres=gpu:8     CPUs=80 Sockets=2 CoresPerSocket=20 ThreadsPerCore=2 Procs=40 RealMemory=489913 State=UNKNOWN
NodeName=dgx-3  Gres=gpu:8     CPUs=80 Sockets=2 CoresPerSocket=20 ThreadsPerCore=2 Procs=40 RealMemory=489913 State=UNKNOWN
NodeName=dgx-4  Gres=gpu:8     CPUs=80 Sockets=2 CoresPerSocket=20 ThreadsPerCore=2 Procs=40 RealMemory=489913 State=UNKNOWN
NodeName=dgx-5  Gres=gpu:8     CPUs=80 Sockets=2 CoresPerSocket=20 ThreadsPerCore=2 Procs=40 RealMemory=489913 State=UNKNOWN
#NodeName=lmd-2  Gres=gpu:8     CPUs=256 Sockets=2 CoresPerSocket=64 ThreadsPerCore=2 Procs=128 RealMemory=4116417 State=UNKNOWN
#NodeName=lmd-3  Gres=gpu:8     CPUs=252 Sockets=2 CoresPerSocket=63 ThreadsPerCore=2 Procs=126 RealMemory=4116352 State=UNKNOWN
#NodeName=lmd-4  Gres=gpu:8     CPUs=256 Sockets=2 CoresPerSocket=64 ThreadsPerCore=2 Procs=128 RealMemory=4116414 State=UNKNOWN
# BIOS settings changed 2023/03/14
NodeName=lmd-1  Gres=gpu:8     CPUs=128 Sockets=2 CoresPerSocket=64 ThreadsPerCore=1 Procs=128 RealMemory=4116381 State=UNKNOWN
NodeName=lmd-2  Gres=gpu:8     CPUs=128 Sockets=2 CoresPerSocket=64 ThreadsPerCore=1 Procs=128 RealMemory=4116383 State=UNKNOWN
NodeName=lmd-3  Gres=gpu:8     CPUs=128 Sockets=2 CoresPerSocket=64 ThreadsPerCore=1 Procs=126 RealMemory=4116352 State=UNKNOWN
#  2024/0620 gpu=6 test
#NodeName=lmd-4  Gres=gpu:8     CPUs=128 Sockets=2 CoresPerSocket=64 ThreadsPerCore=1 Procs=128 RealMemory=4116369 State=UNKNOWN
NodeName=lmd-4  Gres=gpu:1     CPUs=128 Sockets=2 CoresPerSocket=64 ThreadsPerCore=1 Procs=128 RealMemory=4116369 State=UNKNOWN
#NodeName=lmd-4  Gres=gpu:7     CPUs=128 Sockets=2 CoresPerSocket=64 ThreadsPerCore=1 Procs=128 RealMemory=4116381 State=UNKNOWN
#
# PARTITIONS
#
# # hardcoding the partitions and default memory per node
# # TODO: automatically define the partitions by resource
# # TODO: set DefMemPerCPU = TotalMemory / LogicalCPUs
# #PartitionName=batch Nodes=ALL Default=YES DefMemPerCPU=0 State=UP OverSubscribe=NO MaxTime=INFINITE 
#
#PartitionName=Basic Nodes=dgx-[1-5] Qos=Basic AllowQos=Basic MaxTime=00:10:00 DefaultTime=00:10:00 State=UP OverSubscribe=NO 
#PartitionName=Short Nodes=dgx-[1-5] Qos=Short AllowQos=Short MaxTime=01:00:00 DefaultTime=01:00:00 State=UP OverSubscribe=NO
#PartitionName=Medium Nodes=dgx-[1-5] Qos=Medium AllowQos=Medium MaxTime=04:00:00 DefaultTime=04:00:00 State=UP OverSubscribe=NO
#PartitionName=Long Nodes=dgx-[1-5] Qos=Long AllowQos=Long MaxTime=10:00:00 DefaultTime=10:00:00 State=UP OverSubscribe=NO
#PartitionName=Mammoth Nodes=dgx-5 Qos=Mammoth AllowQos=Mammoth MaxTime=14-00:00:00 DefaultTime=14-00:00:00 State=UP OverSubscribe=NO

PartitionName=Basic Nodes=dgx-[1-5]  PriorityTier=1 DefMemPerCPU=1000 MaxMemPerCPU=6000 MaxCPUsPerNode=40 MaxTime=00:10:00 DefaultTime=00:10:00 State=UP OverSubscribe=NO 
PartitionName=Short Nodes=dgx-[1-5]  PriorityTier=1 DefMemPerCPU=1000 MaxMemPerCPU=6000 MaxCPUsPerNode=40 MaxTime=02:00:00 DefaultTime=02:00:00 State=UP DenyAccounts=bwh-comppath-full-g,bwh-comppath-img-g OverSubscribe=NO
PartitionName=Medium Nodes=dgx-[1-5] PriorityTier=1 DefMemPerCPU=1000 MaxMemPerCPU=6000 MaxCPUsPerNode=40 MaxTime=1-00:00:00 DefaultTime=1-00:00:00 State=UP DenyAccounts=bwh-comppath-full-g,bwh-comppath-img-g OverSubscribe=NO
PartitionName=Long Nodes=dgx-[1-5]  PriorityTier=1 DefMemPerCPU=1000 MaxMemPerCPU=6000 MaxCPUsPerNode=40 MaxTime=5-00:00:00 DefaultTime=5-00:00:00 State=UP DenyAccounts=bwh-comppath-full-g,bwh-comppath-img-g OverSubscribe=NO
PartitionName=Mammoth Nodes=dgx-5 PriorityTier=1 DefMemPerCPU=1000 MaxMemPerCPU=6000 MaxCPUsPerNode=40 MaxTime=14-00:00:00 DefaultTime=14-00:00:00 State=UP DenyAccounts=bwh-comppath-full-g,bwh-comppath-img-g OverSubscribe=NO

PartitionName=bwh_comppath-LONG    Nodes=lmd-[1-3]  PriorityTier=1 State=UP AllowGroups=lsfadmin,BWH-COMPPATH-FULL-G,BWH-COMPPATH-IMG-G  MaxTime=28-00:00:00 DefaultTime=1-00:00:00
PartitionName=bwh_comppath   Nodes=lmd-4  PriorityTier=1 State=UP AllowGroups=lsfadmin,BWH-COMPPATH-FULL-G,BWH-COMPPATH-IMG-G  MaxTime=5-00:00:00 DefaultTime=24:00:00
PartitionName=bwh_comppath-Interactive   Nodes=lmd-4  PriorityTier=1 State=UP AllowGroups=lsfadmin,BWH-COMPPATH-FULL-G,BWH-COMPPATH-IMG-G  MaxTime=1-00:00:00 DefaultTime=24:00:00

# Set Srun Portrange
SrunPortRange=60001-60099

# Disable config hash check
DebugFlags=NO_CONF_HASH
